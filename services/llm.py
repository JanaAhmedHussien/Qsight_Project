import requests, json, config

class LLM:
    def generate_report(self, patient, diag):
        for fn in (
            ("ChatGPT", self._chatgpt),
            ("Gemini", self._gemini),
            ("Groq", self._groq),
            ("HuggingFace", self._hf),
        ):
            name, func = fn
            print(f"[LLM] Trying {name}...")
            result = func(patient, diag)
            if result:
                print(f"[LLM]  Response generated by {name}")
                return result

        print("[LLM]  No LLM responded successfully")
        return None


    def _base_prompt(self, patient, diag):
        return {
            "role": "user",
            "content": f"""
You are a healthcare AI assistant.

Return STRICT JSON only. No markdown. No explanations outside JSON.

Patient:
Name: {patient.name}
Age: {patient.age}
Sex: {patient.sex}
Weight: {patient.weight}
Height: {patient.height}
BMI: {patient.bmi}
Insulin: {patient.insulin}
Smoker: {patient.smoker}
Alcohol: {patient.alcohol}
Vascular Disease: {patient.vascular}

Diagnosis:
Left Eye: {diag['retinopathy_left']}
Right Eye: {diag['retinopathy_right']}
Confidence: {diag['confidence']}
Risk Score: {diag['risk']}

JSON schema:
{{
  "condition_overview": "",
  "patient_assessment": "",
  "implications": "",
  "treatment_plan": "",
  "life_impact": "",
  "financial_impact": "",
  "recovery_projection": "",
  "additional_assessments": "",
  "compliance_notice": ""
}}
"""
        }

    # ---------- ChatGPT ----------
    def _chatgpt(self, patient, diag):
        if not config.Config.OPENAI:
            return None
        try:
            r = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {config.Config.OPENAI}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-4o-mini",
                    "messages": [self._base_prompt(patient, diag)],
                    "temperature": 0.3
                },
                timeout=10
            ).json()
            return json.loads(r["choices"][0]["message"]["content"])
        except:
            return None

    # ---------- Gemini ----------
    def _gemini(self, patient, diag):
        if not config.Config.GEMINI:
            return None
        try:
            from google import genai
            c = genai.Client(api_key=config.Config.GEMINI)
            r = c.models.generate_content(
                model="gemini-2.5-flash",
                contents=self._base_prompt(patient, diag)["content"]
            )
            return json.loads(r.text)
        except:
            return None

    # ---------- Groq ----------
    def _groq(self, patient, diag):
        if not config.Config.GROQ:
            return None
        try:
            r = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={"Authorization": f"Bearer {config.Config.GROQ}"},
                json={
                    "model": "llama3-8b-8192",
                    "messages": [self._base_prompt(patient, diag)],
                    "temperature": 0.3
                },
                timeout=10
            ).json()
            return json.loads(r["choices"][0]["message"]["content"])
        except:
            return None

    # ---------- HuggingFace ----------
    def _hf(self, patient, diag):
        if not config.Config.HF:
            return None
        try:
            r = requests.post(
                "https://api-inference.huggingface.co/models/google/flan-t5-base",
                headers={"Authorization": f"Bearer {config.Config.HF}"},
                json={"inputs": self._base_prompt(patient, diag)["content"]},
                timeout=15
            ).json()
            return json.loads(r[0]["generated_text"])
        except:
            return None
